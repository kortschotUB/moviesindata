{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"../assets/plot_styles.mplstyle\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import redis\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import shap\n",
    "\n",
    "sys.path.append('../library')\n",
    "from core import flattenWithGenerator\n",
    "from plotting import loadPalette, loadTableStyles, createBoxplotWithTTests\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import string\n",
    "\n",
    "import cpi\n",
    "from adjustText import adjust_text\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "\n",
    "color_palette = loadPalette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image_path = '../assets/savedImages/howToMakeMoneyWithAHorror'\n",
    "\n",
    "if not os.path.exists(save_image_path):\n",
    "    os.makedirs(save_image_path)\n",
    "\n",
    "TMDB_AUTH_TOKEN = os.getenv('TMDB_AUTH_TOKEN')\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {TMDB_AUTH_TOKEN}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TMDB Df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_df_raw = pd.read_csv('../data/tmdbDetails.csv')\n",
    "tmdb_df_raw.drop_duplicates('imdb_id', keep='last', inplace=True)\n",
    "tmdb_df_raw = tmdb_df_raw[tmdb_df_raw['adult'] == False]\n",
    "tmdb_df_raw = tmdb_df_raw[tmdb_df_raw['genres'].str.contains('Animation') == False]\n",
    "tmdb_df_raw['release_date'] = pd.to_datetime(tmdb_df_raw['release_date'])\n",
    "tmdb_df_raw['year'] = tmdb_df_raw['release_date'].dt.year\n",
    "tmdb_df_raw = tmdb_df_raw[tmdb_df_raw['revenue'] > 0]\n",
    "tmdb_df_raw = tmdb_df_raw[tmdb_df_raw['budget'] > 0]\n",
    "tmdb_df_raw['RBR'] = tmdb_df_raw['revenue'] / tmdb_df_raw['budget']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_df_raw['first_genre'] = tmdb_df_raw['genres'].str.split(\"'name': '\").str[1].str.split(\"'\").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_df_raw = tmdb_df_raw[tmdb_df_raw['first_genre'] != 'TV Movie']\n",
    "# Assuming tmdb_df_raw is your DataFrame\n",
    "grouped = tmdb_df_raw.groupby('first_genre')\n",
    "\n",
    "# Extract the data for each group\n",
    "data = [group['RBR'].values for name, group in grouped]\n",
    "\n",
    "# Create the box plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "medianprops=dict(color='k', linewidth=3)\n",
    "bp = ax.boxplot(data, whis=.75, patch_artist=True, medianprops=medianprops)\n",
    "\n",
    "means = [group['RBR'].median() for name, group in grouped]\n",
    "max_mean_idx = means.index(max(means))\n",
    "bp['boxes'][max_mean_idx].set_facecolor(color_palette['lime'])\n",
    "\n",
    "for i, _ in enumerate(bp['boxes']):\n",
    "    if i == max_mean_idx:\n",
    "        continue\n",
    "    bp['boxes'][i].set_facecolor(color_palette['canvas'])\n",
    "\n",
    "plt.suptitle('')  # Suppress the default title to avoid overlap\n",
    "plt.xlabel('Primary Genre', fontsize=14)\n",
    "plt.ylabel('RBR', fontsize=14)\n",
    "\n",
    "# Set x-axis labels\n",
    "ax.set_xticklabels(grouped.groups.keys())\n",
    "\n",
    "# Set y-axis limit\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_xticklabels(grouped.groups.keys(), rotation=45, ha='right')\n",
    "fig.subplots_adjust(left=0.1, right=0.95, top=0.95, bottom=0.1, hspace=0.2, wspace=0.15)\n",
    "\n",
    "\n",
    "# Save image\n",
    "image_file_path = os.path.join(save_image_path, 'genreMedians.png')\n",
    "plt.savefig(image_file_path, dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for horror & adjust for inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the release_date column\n",
    "tmdb_df_raw['release_year'] = tmdb_df_raw['release_date'].dt.year\n",
    "\n",
    "# Filter the DataFrame for Horror genre\n",
    "horror_df = tmdb_df_raw[tmdb_df_raw['genres'].str.contains('Horror')]\n",
    "\n",
    "# Find adj year\n",
    "horror_df['adjust_to'] = horror_df['release_year'] + ((2024 - horror_df['release_year'])/1.75)\n",
    "\n",
    "horror_df.dropna(subset='adjust_to', inplace=True)\n",
    "\n",
    "# Define a function to adjust a whole column for inflation\n",
    "def adj_for_inflation(column, years):\n",
    "    return [cpi.inflate(value, year, to=2023) if year < 2023 else value for value, year in zip(column, years)]\n",
    "\n",
    "# Adjust the revenue and budget columns for inflation\n",
    "horror_df['revenue_adj'] = adj_for_inflation(horror_df['revenue'], horror_df['adjust_to'].astype(int))\n",
    "horror_df['budget_adj'] = adj_for_inflation(horror_df['budget'], horror_df['adjust_to'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = list(tmdb_df_raw['imdb_id'].unique())\n",
    "\n",
    "with open('../data/all_ids.json', 'w') as f:\n",
    "    json.dump(all_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = list(flattenWithGenerator([ast.literal_eval(e) for e in list(tmdb_df_raw['genres'])]))\n",
    "unique_genres = set([e['name'] for e in all_genres])\n",
    "print(unique_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horror_df['RBR'] = horror_df['revenue'] / horror_df['budget']\n",
    "horror_df['RBR_adj'] = horror_df['revenue_adj'] / horror_df['budget_adj']\n",
    "\n",
    "print(f\"WE HAVE {len(horror_df)} HORROR MOVIES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budget vs. Revenue Scsatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define formatter function\n",
    "def millions(x, pos):\n",
    "    return f'{x * 1e-6:.1f}M'\n",
    "def billions(x, pos):\n",
    "    return f'{x * 1e-9:.1f}B'\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(horror_df['budget_adj'], horror_df['revenue_adj'], c = loadPalette()['blue_grey_dark'], zorder=1)\n",
    "\n",
    "# Define line points\n",
    "x_vals = np.array(ax.get_xlim())\n",
    "y_vals = x_vals  # Slope of one\n",
    "\n",
    "# Plot line\n",
    "ax.plot(x_vals, y_vals, '--', color=loadPalette()['cherry'], label='Revenue = Budget')\n",
    "\n",
    "# Apply formatter to x and y axes\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(millions))\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(billions))\n",
    "ax.set_xlabel('Adjusted Budget', fontsize=14)\n",
    "ax.set_ylabel('Adjusted Revenue', fontsize=14)\n",
    "\n",
    "# Annotate outliers\n",
    "texts = []\n",
    "for i, row in horror_df.iterrows():\n",
    "    if row['RBR'] > 450:\n",
    "        texts.append(ax.text(\n",
    "            row['budget'], row['revenue'], row['title'],\n",
    "            fontsize=8, fontfamily='monospace', alpha=1, zorder=2\n",
    "        ))\n",
    "\n",
    "for i, row in horror_df.iterrows():\n",
    "    if row['revenue_adj'] > 500_000_000:\n",
    "        texts.append(ax.text(\n",
    "            row['budget_adj'], row['revenue_adj'], row['title'],\n",
    "            fontsize=8, fontfamily='monospace', alpha=1, zorder=2\n",
    "        ))\n",
    "\n",
    "# Adjust text to avoid overlap\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray', lw=0.25))\n",
    "        \n",
    "plt.legend()\n",
    "plt.tight_layout(pad=2.0, w_pad=0.5, h_pad=1.0)\n",
    "\n",
    "# Save image\n",
    "image_file_path = os.path.join(save_image_path, 'xyScatterAll.png')\n",
    "plt.savefig(image_file_path, dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Film Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/horror_classifications.json') as f:\n",
    "    classifications = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = pd.DataFrame.from_dict(classifications).T\n",
    "scores_df = pd.json_normalize(class_df[2])\n",
    "scores_df.index = class_df.index\n",
    "\n",
    "class_df.drop(2, axis=1, inplace=True)\n",
    "\n",
    "class_df = pd.merge(class_df, scores_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbr_dict = horror_df.set_index('imdb_id')['RBR'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df['RBR'] = class_df.index.map(rbr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df.rename(columns={0:'classification',1:'confidence'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df.dropna(subset='RBR', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = class_df.groupby('classification')\n",
    "\n",
    "# Extract the data for each group\n",
    "data = [group['RBR'].values for name, group in grouped]\n",
    "\n",
    "\n",
    "means = [group['RBR'].median() for name, group in grouped]\n",
    "maxMeanIdx = means.index(max(means))\n",
    "bp['boxes'][maxMeanIdx].set_facecolor(color_palette['lime'])\n",
    "\n",
    "# Create the box plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "medianprops=dict(color='k', linewidth=3)\n",
    "bp = ax.boxplot(data, whis=.75, patch_artist=True, medianprops=medianprops)\n",
    "\n",
    "for i, _ in enumerate(bp['boxes']):\n",
    "    bp['boxes'][i].set_facecolor(color_palette['canvas'])\n",
    "\n",
    "plt.suptitle('')  # Suppress the default title to avoid overlap\n",
    "plt.xlabel('Primary Classification', fontsize=14)\n",
    "plt.ylabel('RBR', fontsize=14)\n",
    "\n",
    "# Set x-axis labels\n",
    "ax.set_xticklabels(grouped.groups.keys())\n",
    "\n",
    "# Set y-axis limit\n",
    "ax.set_ylim(0, 15)\n",
    "ax.set_xticklabels(grouped.groups.keys(), rotation=45, ha='right')\n",
    "fig.subplots_adjust(left=0.1, right=0.95, top=0.95, bottom=0.1, hspace=0.2, wspace=0.15)\n",
    "\n",
    "# Save image\n",
    "image_file_path = os.path.join(save_image_path, 'classification_medians.png')\n",
    "plt.savefig(image_file_path, dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG BOOST\n",
    "- We're going to fit an xgboost model with several possibly meaningful features\n",
    "- Then we'll look at the shapley values that are returned from those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, root_mean_squared_error  # For classification tasks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "r5 = redis.Redis(\n",
    "    host='127.0.0.1',\n",
    "    port=6379,\n",
    "    charset=\"utf-8\",\n",
    "    decode_responses=True,\n",
    "    db=5\n",
    ")\n",
    "\n",
    "redis_keys = r5.keys('*')\n",
    "redis_values = [float(i) for i in r5.mget(redis_keys) if i != 'None']\n",
    "\n",
    "ratings_dict = dict(zip(redis_keys,redis_values))\n",
    "\n",
    "horror_df['imdb_rating'] = horror_df['imdb_id'].map(ratings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "r6 = redis.Redis(\n",
    "    host='127.0.0.1',\n",
    "    port=6379,\n",
    "    charset=\"utf-8\",\n",
    "    decode_responses=True,\n",
    "    db=6\n",
    ")\n",
    "\n",
    "redis_keys = r6.keys('*')\n",
    "redis_values = [json.loads(i) for i in r6.mget(redis_keys)]\n",
    "\n",
    "cast_dict = dict(zip(redis_keys,redis_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "horror_ids = set(horror_df['imdb_id'].unique())\n",
    "cast_dict = {imdb_id: people for imdb_id, people in cast_dict.items() if imdb_id in horror_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_credits_dict = {}\n",
    "\n",
    "for imdb_id, people in cast_dict.items():\n",
    "    filtered_credits_dict[imdb_id] = {\n",
    "        'top_credits': [{'name':a['name'], 'id':a['id'], 'gender':a['gender']} for a in people['cast'] if a['order'] < 6],\n",
    "        'directors': [{'name':d['name'], 'id':d['id'], 'gender':d['gender']} for d in people['crew'] if d['job'] == 'Director']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_dates_dict = horror_df.set_index('imdb_id')['release_date'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imdb_id, people in tqdm(filtered_credits_dict.items()):\n",
    "    try:\n",
    "        release_date = pd.to_datetime(release_dates_dict[imdb_id], utc=True)\n",
    "        top_credits_updated = []\n",
    "        directors_credits_updated = []\n",
    "        for person in people['top_credits']:\n",
    "            try:\n",
    "                person_id = person['id']\n",
    "                \n",
    "                url = f\"https://api.themoviedb.org/3/person/{person_id}?language=en-US\"\n",
    "                details = requests.get(url, headers=headers)\n",
    "\n",
    "                birthday = pd.to_datetime(details.json()['birthday'], utc=True)\n",
    "\n",
    "                age = relativedelta(release_date, birthday).years\n",
    "                person['age'] = age\n",
    "\n",
    "                top_credits_updated.append(person)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        for person in people['directors']:\n",
    "            try:\n",
    "                person_id = person['id']\n",
    "                \n",
    "                url = f\"https://api.themoviedb.org/3/person/{person_id}?language=en-US\"\n",
    "                details = requests.get(url, headers=headers)\n",
    "\n",
    "                birthday = pd.to_datetime(details.json()['birthday'], utc=True)\n",
    "\n",
    "                age = relativedelta(release_date, birthday).years\n",
    "\n",
    "                person['age'] = age\n",
    "\n",
    "                directors_credits_updated.append(person)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        filtered_credits_dict[imdb_id] = {\n",
    "            'directors': directors_credits_updated,\n",
    "            'top_credits': top_credits_updated\n",
    "        }\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_features_dict = {}\n",
    "\n",
    "people_ids_dict = {}\n",
    "\n",
    "for imdb_id, people in filtered_credits_dict.items():\n",
    "    if len(people['directors']) == 0:\n",
    "        continue\n",
    "    if len(people['top_credits']) == 0:\n",
    "        continue\n",
    "\n",
    "    director_age = np.mean([p['age'] for p in people['directors']])\n",
    "    director_gender = people['directors'][0]['gender']\n",
    "    \n",
    "    cast_mean_age = np.mean([p['age'] for p in people['top_credits']])\n",
    "    lead_gender = people['top_credits'][0]['gender']\n",
    "\n",
    "    imdb_features_dict[imdb_id] = {\n",
    "        'director_age': director_age,\n",
    "        'director_gender': director_gender,\n",
    "        'cast_mean_age': cast_mean_age,\n",
    "        'lead_gender': lead_gender\n",
    "    }\n",
    "\n",
    "    people_ids_dict[imdb_id] = {\n",
    "        'cast_encoded': [i['id'] for i in people['top_credits']],\n",
    "        'director_encoded': [i['id'] for i in people['directors']][0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_dict(people_ids_dict).T\n",
    "data.rename(columns = {'index':'imdb_id'}, inplace=True)\n",
    "\n",
    "unique_cast_ids =  sorted(set(list(flattenWithGenerator(data['cast_encoded']))))\n",
    "unique_director_ids = sorted(set(list(flattenWithGenerator(data['director_encoded']))))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "horror_df['release_month'] = horror_df['release_date'].dt.month\n",
    "horror_df.set_index('imdb_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_features = pd.DataFrame.from_dict(imdb_features_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.merge(data, horror_df[['imdb_rating','RBR_adj','runtime','release_month']], left_index=True, right_index=True)\n",
    "\n",
    "model_df = pd.merge(imdb_features, model_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_sequential_map = {i: idx for idx,i in enumerate(unique_cast_ids)}\n",
    "director_sequential_map = {i: idx for idx,i in enumerate(unique_director_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare the data\n",
    "# Assuming data is your DataFrame as defined earlier\n",
    "model_df['director_encoded'] = pd.to_numeric(model_df['director_encoded'], errors='coerce')  # Convert to numeric, coerce errors\n",
    "model_df['cast_encoded'] = model_df['cast_encoded'].apply(lambda x: [cast_sequential_map[int(i)] for i in x])  # Ensure cast_encoded is of int type\n",
    "model_df['director_encoded'] = model_df['director_encoded'].apply(lambda x: director_sequential_map[x])\n",
    "\n",
    "\n",
    "max_cast_id = max([max(cast) for cast in model_df['cast_encoded']])\n",
    "num_casts = len(set([item for sublist in model_df['cast_encoded'] for item in sublist]))  # Unique cast ids\n",
    "num_directors = model_df['director_encoded'].nunique()  # Unique director ids\n",
    "\n",
    "# Create input tensor for cast and director\n",
    "cast_tensors = torch.nn.utils.rnn.pad_sequence(\n",
    "    [torch.tensor(cast) for cast in model_df['cast_encoded']],\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "\n",
    "# Check if there are any NaN values after conversion\n",
    "if model_df['director_encoded'].isnull().any():\n",
    "    print(\"Warning: There are NaN values in director_encoded after conversion.\")\n",
    "\n",
    "# Convert director_encoded to a tensor\n",
    "director_tensors = torch.tensor(model_df['director_encoded'].values.astype(int))  # Ensure it's int\n",
    "# Prepare target tensor\n",
    "targets = torch.tensor(model_df['RBR_adj'].values).float()\n",
    "\n",
    "# Check tensors\n",
    "print(\"Cast Tensor Shape:\", cast_tensors.shape)\n",
    "print(\"Director Tensor Shape:\", director_tensors.shape)\n",
    "print(\"Targets Shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cast_id = max([max(cast) for cast in model_df['cast_encoded']])\n",
    "max_director_id = max(model_df['director_encoded'])\n",
    "print(\"Max cast ID:\", max_cast_id)\n",
    "print(\"Max director ID:\", max_director_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding model\n",
    "class MovieEmbeddingModel(nn.Module):\n",
    "    def __init__(self, num_actors, num_directors, embedding_dim):\n",
    "        super(MovieEmbeddingModel, self).__init__()\n",
    "        self.actor_embedding = nn.Embedding(num_actors, embedding_dim)\n",
    "        self.director_embedding = nn.Embedding(num_directors, embedding_dim)\n",
    "\n",
    "    def forward(self, actor_ids, director_ids):\n",
    "        # Get embeddings for actors\n",
    "        actor_embeds = self.actor_embedding(actor_ids)  # Shape: (batch_size, num_actors, embedding_dim)\n",
    "        # Average the actor embeddings\n",
    "        actor_avg_embedding = actor_embeds.mean(dim=1)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "        # Get the director embedding\n",
    "        director_embed = self.director_embedding(director_ids)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "        # Combine actor and director embeddings\n",
    "        combined_embedding = actor_avg_embedding + director_embed\n",
    "        return combined_embedding\n",
    "\n",
    "# Training parameters\n",
    "embedding_dim =  cast_tensors.shape[0]  # Size of the embeddings\n",
    "model = MovieEmbeddingModel(num_actors=num_casts + 1, num_directors=num_directors + 1, embedding_dim=embedding_dim)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "num_epochs = 100  # Adjust based on your needs\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(cast_tensors, director_tensors)  # Forward pass\n",
    "    loss = criterion(outputs.squeeze(), targets)  # Calculate loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Optimize\n",
    "    if (epoch + 1) % 10 == 0:  # Print every 10 epochs\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Extract embeddings\n",
    "actor_embeddings = model.actor_embedding.weight.data.numpy()\n",
    "director_embeddings = model.director_embedding.weight.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_embeddings = np.random.rand(model_df.shape[0], cast_tensors.shape[0])  # Replace with actual embedding size and values\n",
    "director_embeddings = np.random.rand(model_df.shape[0], cast_tensors.shape[0])  # Same here\n",
    "\n",
    "feature_cols = ['director_age', 'director_gender', 'cast_mean_age', 'lead_gender', 'imdb_rating', 'runtime', 'release_month']\n",
    "\n",
    "means = model_df[feature_cols].mean()\n",
    "\n",
    "model_df.fillna(means, inplace=True)\n",
    "\n",
    "# Combine features\n",
    "features = np.hstack((\n",
    "    model_df[feature_cols].values,\n",
    "    actor_embeddings,\n",
    "    director_embeddings\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Normalizer()\n",
    "\n",
    "features_normed = scaler.fit_transform(features)\n",
    "\n",
    "X = features\n",
    "y = model_df['RBR_adj'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],     # Number of trees\n",
    "    'max_depth': [3, 5, 7],              # Depth of trees\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Learning rate\n",
    "    'min_child_weight': [1, 3, 5],       # Minimum child weight\n",
    "    'gamma': [0, 0.1, 0.2],              # Minimum loss reduction to split\n",
    "    'subsample': [0.8, 1.0],             # Fraction of data for boosting\n",
    "    'colsample_bytree': [0.8, 1.0],      # Fraction of features for each tree\n",
    "    'reg_alpha': [0.1, 0.5],             # L1 regularization term\n",
    "    'reg_lambda': [1.0, 2.0],            # L2 regularization term\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "xgb_reg = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, scoring='neg_mean_squared_error', \n",
    "                        cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "# X_train_d = dtrain.get_float_info('data')\n",
    "# y_train_d = dtrain.get_label()\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Evaluate using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = np.sqrt(root_mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE after tuning: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer for your XGBoost model\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(dtrain)\n",
    "\n",
    "# Plot a summary of feature importance with direction of effect\n",
    "plt.figure()\n",
    "\n",
    "fig = shap.summary_plot(shap_values, dtrain, show=False)\n",
    "image_file_path = os.path.join(save_image_path, 'shapValues.png')\n",
    "plt.savefig(image_file_path, dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(5, shap_values, X_test)  # 0 refers to the index of the feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
