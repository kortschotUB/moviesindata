{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../library/')\n",
    "from core import extractBetween, extractElementsInOrder, exceptionOutput, createSlidingWindows\n",
    "from midStats import \n",
    "from datetime import datetime\n",
    "from uuid import uuid1\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from datetime import timedelta\n",
    "import arrow\n",
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm   \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Redis Data from DB 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = redis.Redis(\n",
    "        host='localhost',\n",
    "        port=6379,\n",
    "        charset=\"utf-8\",\n",
    "        decode_responses=True,\n",
    "        db = 3\n",
    "    )\n",
    "\n",
    "# TARGET DF\n",
    "dates = r.keys()\n",
    "vals  = r.mget(dates)\n",
    "\n",
    "valsJson = [el[1:] for el in [json.loads(e) for e in vals]]\n",
    "\n",
    "columns = ['rank','previous','title','distributor','weekGross','pctLastWeek','numberOfTheaters','numberOfTheatersChange','perTheaterAvg','totalGross','weeksInRelease']\n",
    "\n",
    "allWeeks = []\n",
    "for i, week in enumerate(tqdm(valsJson)):\n",
    "    for movie in week:\n",
    "        if all([v == '' for v in movie]) | (movie[0] == ''):\n",
    "            continue\n",
    "\n",
    "        movieDict = dict(zip(columns, movie))\n",
    "        movieDict['date'] = dates[i]\n",
    "        allWeeks.append(movieDict)\n",
    "\n",
    "df = pd.DataFrame.from_dict(allWeeks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dateDt'] = pd.to_datetime(df['date'], utc=True)\n",
    "\n",
    "for col in ['weekGross','pctLastWeek','numberOfTheaters','perTheaterAvg','totalGross']:\n",
    "    df[col] = df[col].str.replace('$','', regex=False)\\\n",
    "                                    .str.replace(',','',regex=False)\\\n",
    "                                    .str.replace('%','',regex=False)\\\n",
    "                                    .str.replace('<','',regex=False)\\\n",
    "                                    .str.replace('(v)', '', regex=False)\\\n",
    "                                    .replace('-', np.nan)\\\n",
    "                                    .replace('', np.nan)\\\n",
    "                                    .replace('n/c', np.nan)\\\n",
    "                                    .astype(float)\n",
    "\n",
    "df.sort_values(by=['dateDt', 'weekGross'], ascending=[False, False], inplace=True)\n",
    "\n",
    "# This step is required to infer the release date\n",
    "l1 = len(df)\n",
    "df.drop_duplicates(subset=['title','distributor'], keep='last', inplace=True)\n",
    "print(f\"WE DROPPED: {l1-len(df)} DUPLICATES\")\n",
    "\n",
    "df['uuid'] = df.apply(lambda x: str(uuid1()), axis=1)\n",
    "targetDict = df[['uuid','title','dateDt']].to_dict('records')\n",
    "\n",
    "# CANDIDATE DF\n",
    "tmdbDf = pd.read_csv('../data/tmdbDetails.csv')\n",
    "relDf = tmdbDf[['title', 'release_date','imdb_id']]\n",
    "relDf['release_date'] = pd.to_datetime(relDf['release_date'], utc=True)\n",
    "relDf.drop_duplicates(subset='imdb_id', keep='last', inplace=True)\n",
    "relDf.dropna(subset='release_date', inplace=True)\n",
    "candidateDict = relDf.to_dict('records')\n",
    "\n",
    "# Set up threading\n",
    "cores = os.cpu_count()\n",
    "pool = Pool(cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Windows & Vectorize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateDict = [{'title':e['title'].lower(),'dateDt':e['release_date'], 'imdbId':e['imdb_id']} for e in candidateDict if e['release_date'] <= arrow.now()]\n",
    "print(f\"THERE ARE: {len(candidateDict)} CANDIDATES TO CHOOSE FROM\")\n",
    "\n",
    "allTargets = [{'title':e['title'].lower(),'dateDt':e['dateDt'], 'uuid':e['uuid']} for e in targetDict]\n",
    "\n",
    "# Sort the lists according to date in reverse order\n",
    "candidateDict = sorted(candidateDict, key=lambda x: x['dateDt'], reverse = True)\n",
    "allTargetsRaw = sorted(allTargets, key=lambda x: x['dateDt'], reverse = True)\n",
    "\n",
    "allTargets = []\n",
    "\n",
    "for target in allTargetsRaw:\n",
    "    if target not in allTargets:\n",
    "        allTargets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort candidateDict once\n",
    "candidateDict = sorted(candidateDict, key=lambda x: x['dateDt'], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find perfect matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get years for candidate dict and target dict\n",
    "for t in tqdm(allTargets):\n",
    "    t['year'] = t['dateDt'].year\n",
    "\n",
    "for c in tqdm(candidateDict):\n",
    "    c['year'] = c['dateDt'].year\n",
    "\n",
    "notFound = []\n",
    "\n",
    "idMappings = {}\n",
    "\n",
    "for t in tqdm(allTargets):\n",
    "    foundIds = {c['imdbId'] for c in candidateDict if c['title'] == t['title'] and c['year'] == t['year']}\n",
    "    if len(foundIds) != 1:\n",
    "        notFound.append(t)\n",
    "\n",
    "    else:\n",
    "        idMappings[t['uuid']] = {\n",
    "            'imdbId': foundIds,\n",
    "            'title': t['title']\n",
    "        }\n",
    "\n",
    "for i, d in idMappings.items():\n",
    "    d['imdbId'] = list(d['imdbId'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/theNumbersData/perfectMatches.json', 'w') as f:\n",
    "    json.dump(idMappings, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best guess at imperfect matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundIds = set([i['imdbId'] for i in tqdm(idMappings.values())])\n",
    "candidatesFiltered = [c for c in candidateDict if c['imdbId'] not in foundIds]\n",
    "\n",
    "print(f\"NEW LENGTH OF CANDIDATES IS: {len(candidatesFiltered)}\")\n",
    "\n",
    "\n",
    "windowSize = 50\n",
    "targetWindows = createSlidingWindows(notFound, windowSize)\n",
    "print(f\"WE ARE PROCESSING: {len(targetWindows)} WINDOWS OF {windowSize}\")\n",
    "print(f\"TOTAL SEARCH SPACE IS {len(notFound) * len(candidatesFiltered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allMatches = []\n",
    "\n",
    "def find_best_match_parallel(targetDict, candidateDict, vectors_L2, vectorizer):\n",
    "    targetDict = sorted(targetDict, key=lambda x: x['dateDt'], reverse=True)\n",
    "    releaseRange = 90\n",
    "    minTarget = min(targetDict, key=lambda x: x['dateDt'])['dateDt'] - timedelta(days=releaseRange)\n",
    "    maxTarget = max(targetDict, key=lambda x: x['dateDt'])['dateDt'] + timedelta(days=releaseRange)\n",
    "    \n",
    "    candidateDictFiltered = [c for c in candidateDict if minTarget <= c['dateDt'] <= maxTarget]\n",
    "\n",
    "    # Vectorize titles once\n",
    "    titles_L2 = [item['title'] for item in candidateDictFiltered]\n",
    "    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3,3)).fit(titles_L2)\n",
    "    vectors_L2 = vectorizer.transform(titles_L2)\n",
    "    \n",
    "    print(f\"Number of candidates within our release range: {len(candidateDictFiltered)}\")\n",
    "\n",
    "    titles_L1 = [item['title'] for item in targetDict]\n",
    "    vectors_L1 = vectorizer.transform(titles_L1)\n",
    "    \n",
    "    matches = []\n",
    "    for targetIdx, target in tqdm(enumerate(targetDict), total=len(targetDict)):\n",
    "        best_match = None\n",
    "        highest_score = -1000000\n",
    "        \n",
    "        for i, candidate in enumerate(candidateDictFiltered):\n",
    "            title_score = cosine_similarity(vectors_L1[targetIdx], vectors_L2[i])[0][0]\n",
    "            if title_score > .95:\n",
    "                best_match = candidate\n",
    "                highest_score = 500\n",
    "                break\n",
    "\n",
    "            date_score = abs((target['dateDt'] - candidate['dateDt']).days)\n",
    "            total_score = (400 * title_score) - date_score\n",
    "            \n",
    "            if total_score > highest_score:\n",
    "                highest_score = total_score\n",
    "                best_match = candidate\n",
    "            \n",
    "            if total_score > 320:\n",
    "                break\n",
    "        \n",
    "        matches.append((target, best_match, highest_score))\n",
    "    \n",
    "    \n",
    "    print(f\"We found {len(matches)} good matches!\")\n",
    "    print(f'\\n')\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startingPoint = len([e for e in os.listdir('../data/theNumbersData/') if 'allMatches' in e])\n",
    "\n",
    "windowedWindows = createSlidingWindows(targetWindows[startingPoint:], windowSize = 8, overlap = 0)\n",
    "\n",
    "print(f\"WE HAVE {len(windowedWindows)} WINDOWED WINDOWS TO GO!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startingSaveValue = int([e for e in os.listdir('../data/theNumbersData/') if 'allMatches' in e][0].split('_')[1]) + 1\n",
    "print(f\"STARTING SAVE VALUE: {startingSaveValue}\")\n",
    "\n",
    "for i, targetWindow in enumerate(windowedWindows, start=startingSaveValue):\n",
    "    # Parallel processing\n",
    "    # allMatches = Parallel(n_jobs=-1)(delayed(find_best_match_parallel)(targetDict, candidatesFiltered, vectors_L2, vectorizer) for targetDict in targetWindow)\n",
    "    \n",
    "    for j, targetWindow2 in enumerate(targetWindow):\n",
    "        allMatches = find_best_match_parallel(targetDict = targetWindow2, candidateDict = candidatesFiltered, vectors_L2 = vectors_L2, vectorizer = vectorizer)\n",
    "        # Flatten the list of matches\n",
    "        allMatches = [match for sublist in allMatches for match in sublist]\n",
    "\n",
    "        # Custom serialization function\n",
    "        def custom_serializer(obj):\n",
    "            if isinstance(obj, (np.datetime64, pd.Timestamp)):\n",
    "                return obj.isoformat()\n",
    "            raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "        # Assuming allMatches is already defined and contains the data you want to write\n",
    "        with open(f'../data/theNumbersData/allMatches_{i}_{j}.json', 'w') as f:\n",
    "            json.dump(allMatches, f, default=custom_serializer, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allImperfect = []\n",
    "\n",
    "for file in [i for i in os.listdir('../data/theNumbersData/') if 'allMatches' in i]:\n",
    "    with open(os.path.join('../data/theNumbersData/',file)) as f:\n",
    "        matchesTemp = json.load(f)\n",
    "        allImperfect += matchesTemp\n",
    "\n",
    "# There was an error in saving, so for some reason it created a flat list. We can just create len 3 sliding windows\n",
    "allImperfectGrouped = createSlidingWindows(l = allImperfect, windowSize = 3)\n",
    "\n",
    "goodMatches = sorted([e for e in allImperfectGrouped if e[-1] > 245], key = lambda x: x[-1], reverse=True)\n",
    "badMatches = sorted([e for e in allImperfectGrouped if e[-1] <= 245], key = lambda x: x[-1], reverse=True)\n",
    "\n",
    "print(f\"WE HAVE: {len(goodMatches)} GOOD MATCHES AND WE HAVE: {len(badMatches)} BAD MATCHES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final pass on bad matches before accepting defeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "medMatches = [cosSimWords(t[0]['title'].replace('&','and').replace('’',\"'\"), t[1]['title'].replace('&','and').replace('’',\"'\")) for t in badMatches if t[-1]]\n",
    "testMatches = [item for item, keep in zip(badMatches, medMatches) if keep]\n",
    "\n",
    "goodMatches += testMatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine imperfect and perfect matches and then map to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/theNumbersData/perfectMatches.json') as f:\n",
    "    perfectMatches = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodMatchesMapped = {}\n",
    "\n",
    "for el in goodMatches:\n",
    "    goodMatchesMapped[el[0]['uuid']] = {\n",
    "        'imdbId': el[1]['imdbId'],\n",
    "        'title': el[0]['title']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "allMatchesMapped = {**goodMatchesMapped, **perfectMatches}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "r4 = redis.Redis(\n",
    "        host='localhost',\n",
    "        port=6379,\n",
    "        charset=\"utf-8\",\n",
    "        decode_responses=True,\n",
    "        db = 4\n",
    "    )\n",
    "\n",
    "for uuid, data in allMatchesMapped.items():\n",
    "    r4.set(uuid, json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "allIdsMapped = {uuid: el['imdbId'] for uuid,el in allMatchesMapped.items()}\n",
    "\n",
    "df['imdbId'] = df['uuid'].map(allIdsMapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/numbersBoxOffice.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
