{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/seankortschot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/seankortschot/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/seankortschot/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels as sm\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"../assets/plot_styles.mplstyle\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import redis\n",
    "import string\n",
    "sys.path.append('../library')\n",
    "from core import prettyPrint, getLengthOfDict, makeSingular, flattenWithGenerator\n",
    "from midStats import linearModelGeneral\n",
    "from plotting import loadPalette, loadTableStyles, createBoxplotWithTTests\n",
    "import pandas as pd\n",
    "import holoviews as hv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from holoviews import opts, dim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from typing import Dict, List\n",
    "import nest_asyncio\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import mobypy as mbp\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from dotenv import load_dotenv\n",
    "from nltk.corpus import wordnet\n",
    "from ast import literal_eval\n",
    "import gensim\n",
    "load_dotenv()\n",
    "\n",
    "colorPalette = loadPalette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('../data/topicModelling/GoogleNews-vectors-negative300.bin.gz', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be unambiguously about the thing\n",
    "# For example: Spirit doesn't work for haunting as it will yield syns like zest and energy\n",
    "# You can just include specific keywords here as well (e.g., godzilla); with the kwWeight they will yield high scores\n",
    "class ConceptMap:\n",
    "    \"\"\" Concept Map\n",
    "        - Creates a weighted list of dictionaries that have the core concepts that we are looking for\n",
    "    \"\"\"\n",
    "    def __init__(self, kwWeight):\n",
    "        self.kwWeight = kwWeight\n",
    "        self.conceptMap = []\n",
    "\n",
    "    def create(self, concepts):\n",
    "        for concept, words in concepts.items():\n",
    "            self.conceptMap.append({(concept, self.kwWeight): [(word, self.kwWeight) for word in words]})\n",
    "\n",
    "concepts = {\n",
    "    'haunting': ['devil','spirit','specter', 'satan', 'underworld', 'exorcist','priest', 'ghost', 'curse'],\n",
    "    'killer': ['murder', 'victim', 'sadistic', 'stalker', 'crime', 'detective', 'psychopath'],\n",
    "    'isolation': ['house', 'abandoned', 'cabin', 'desolate', 'forest', 'alone','dessert','mountain'],\n",
    "    'trauma': ['psychological', 'mystery','disturbing','atmosphere', 'moody', 'depression','sadness','turmoil'],\n",
    "    'monster': ['beast', 'godzilla', 'shark', 'alien', 'werewolf', 'vampire', 'spider']\n",
    "}\n",
    "\n",
    "conceptMap = ConceptMap(kwWeight = 0.00001)\n",
    "conceptMap.create(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-embedding\n",
      "  Using cached bert_embedding-1.0.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting typing==3.6.6 (from bert-embedding)\n",
      "  Using cached typing-3.6.6-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting numpy==1.14.6 (from bert-embedding)\n",
      "  Using cached numpy-1.14.6.zip (4.9 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of bert-embedding to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting bert-embedding\n",
      "  Using cached bert_embedding-1.0.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached bert_embedding-0.1.5-py3-none-any.whl.metadata (5.3 kB)\n",
      "  Using cached bert_embedding-0.1.4-py3-none-any.whl.metadata (5.3 kB)\n",
      "  Using cached bert_embedding-0.1.3-py3-none-any.whl.metadata (5.4 kB)\n",
      "  Using cached bert_embedding-0.1.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached bert_embedding-0.1.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "  Using cached bert_embedding-0.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "INFO: pip is still looking at multiple versions of bert-embedding to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Cannot install bert-embedding==0.1.0, bert-embedding==0.1.1, bert-embedding==0.1.2, bert-embedding==0.1.3, bert-embedding==0.1.4, bert-embedding==0.1.5, bert-embedding==1.0.0 and bert-embedding==1.0.1 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    bert-embedding 1.0.1 depends on mxnet==1.4.0\n",
      "    bert-embedding 1.0.0 depends on mxnet==1.4.0\n",
      "    bert-embedding 0.1.5 depends on mxnet==1.4.0\n",
      "    bert-embedding 0.1.4 depends on mxnet==1.3.0\n",
      "    bert-embedding 0.1.3 depends on mxnet==1.3.0\n",
      "    bert-embedding 0.1.2 depends on mxnet==1.3.0\n",
      "    bert-embedding 0.1.1 depends on mxnet==1.3.0\n",
      "    bert-embedding 0.1.0 depends on mxnet==1.3.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bert-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mxnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmxnet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmx\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert_embedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertEmbedding\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mxnet'"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m storage \u001b[38;5;241m=\u001b[39m ContextNeighborStorage(sentences\u001b[38;5;241m=\u001b[39mall_sentences, model\u001b[38;5;241m=\u001b[39mbert)\n\u001b[1;32m      2\u001b[0m storage\u001b[38;5;241m.\u001b[39mprocess_sentences()\n\u001b[1;32m      3\u001b[0m storage\u001b[38;5;241m.\u001b[39mbuild_search_index()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "all_sentences = ['wow cool so cool', 'hey this is cool', 'i have sadness']\n",
    "storage = ContextNeighborStorage(sentences=all_sentences, model=bert)\n",
    "storage.process_sentences()\n",
    "storage.build_search_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAdjacentTerms(term: str = ''):\n",
    "    adjTerms = set()\n",
    "    \n",
    "    synsets = wn.synsets(term, pos=wn.NOUN)\n",
    "    \n",
    "    for synset in synsets:\n",
    "        # Explore hypernyms (more general terms)\n",
    "        for hypernym in synset.hypernyms():\n",
    "            for hyponym in hypernym.hyponyms():\n",
    "                for lemma in hyponym.lemmas():\n",
    "                    adjTerms.add(lemma.name())\n",
    "        \n",
    "        # Explore hyponyms (more specific terms)\n",
    "        for hyponym in synset.hyponyms():\n",
    "            for lemma in hyponym.lemmas():\n",
    "                adjTerms.add(lemma.name())\n",
    "\n",
    "    return [t.replace('_',' ') for t in list(adjTerms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'wv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mmost_similar(positive\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfarts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'wv'"
     ]
    }
   ],
   "source": [
    "model.wv.most_similar(positive='farts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performInitialCheck(conceptMap: list = []):\n",
    "    output = []\n",
    "    for concept in conceptMap:\n",
    "        conceptKws = list(flattenWithGenerator(concept.values()))\n",
    "        nAdj = len(list(flattenWithGenerator([findAdjacentTerms(t[0]) for t in conceptKws])))\n",
    "        output.append((list(concept.keys())[0][0], nAdj))\n",
    "\n",
    "    print(tabulate(output, headers=['Term', 'Starting Adjacency']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performInitialCheck(conceptMap.conceptMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply nest_asyncio to allow nested event loops\n",
    "def constructAdjacencyGraph(category: dict, curDepth: int, maxDepth: int, curCat: str = '', simThresh: float = .6, foundSyns: list = []):\n",
    "    \"\"\" Construct Adjacency Graph\n",
    "        - Creates a synGraph based on a conceptMap\n",
    "    \"\"\"\n",
    "    if curDepth > maxDepth:\n",
    "        return category\n",
    "    \n",
    "    keyDict = {}\n",
    "    \n",
    "    for key, rootSynList in category.items():\n",
    "        rootSynDict = {}\n",
    "        \n",
    "        for rootSyn in rootSynList:\n",
    "            # We only want to use our initial category title and the keywords that we pass in\n",
    "            if curDepth < 2:\n",
    "                curCat = key[0]\n",
    "\n",
    "            if rootSyn[0] not in model:\n",
    "                print(f\"Error: {key[0]}-{rootSyn[0]} not in model\")\n",
    "                continue\n",
    "            \n",
    "            adjTerms = findAdjacentTerms(rootSyn[0])\n",
    "            \n",
    "            similarities = [model.similarity(rootSyn[0], s) if s in model else 0 for s in adjTerms]\n",
    "\n",
    "            synsFiltered = [\n",
    "                (s, 1/similarities[i]) for i, s in enumerate(adjTerms) if (\n",
    "                    (s not in foundSyns) and\n",
    "                    (s in model) and \n",
    "                    (similarities[i] > simThresh)\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            foundSyns += [s for s,_ in synsFiltered]\n",
    "            rootSynDict[rootSyn] = synsFiltered\n",
    "\n",
    "        keyDict[key] = rootSynDict\n",
    "\n",
    "    # Recursively find synonyms for the next depth level\n",
    "    for key in keyDict:\n",
    "        for rootSyn in keyDict[key]:\n",
    "            keyDict[key][rootSyn] = constructAdjacencyGraph(category = {rootSyn: keyDict[key][rootSyn]}, curDepth = curDepth + 1, maxDepth = maxDepth, curCat = curCat, simThresh = simThresh, foundSyns = foundSyns)[rootSyn]\n",
    "    \n",
    "    \n",
    "    return keyDict\n",
    "\n",
    "synGraph = {}\n",
    "\n",
    "for conceptDict in tqdm(conceptMap.conceptMap):\n",
    "    category = list(conceptDict.keys())[0][0]\n",
    "\n",
    "    synGraph[category] = constructAdjacencyGraph(\n",
    "        category  = conceptDict, \n",
    "        curDepth  = 0, \n",
    "        maxDepth  = 15, \n",
    "        curCat    = list(conceptDict.keys())[0], \n",
    "        simThresh = .4,\n",
    "        foundSyns = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of all elements\n",
    "lengths = [(k, getLengthOfDict(v)) for k,v in synGraph.items()]\n",
    "print(tabulate(lengths, headers=['class','syns']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPairs(d, parent=None, value=None, allPairs=None):\n",
    "    if allPairs is None:\n",
    "        allPairs = []\n",
    "\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            if parent is not None:\n",
    "                allPairs.append({'source': parent, 'target': k, 'value': value})\n",
    "            extractPairs(v, k, value if value is not None else parent, allPairs)\n",
    "    elif isinstance(d, list):\n",
    "        for item in d:\n",
    "            allPairs.append({'source': parent, 'target': item, 'value': value})\n",
    "\n",
    "    return allPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPairs = extractPairs(synGraph)\n",
    "\n",
    "for p in allPairs:\n",
    "    if p['value'] == None:\n",
    "        p['value'] = p['source']\n",
    "    \n",
    "    if not isinstance(p['value'], tuple):\n",
    "        p['value'] = (p['value'], 30)\n",
    "    if not isinstance(p['target'], tuple):\n",
    "        p['target'] = (p['target'], 30)\n",
    "    if not isinstance(p['source'], tuple):\n",
    "        p['source'] = (p['source'], 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPairsDf = pd.DataFrame.from_dict(allPairs)\n",
    "\n",
    "allPairsDf.to_csv('../data/horrorTaxonomy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphDf = allPairsDf.applymap(lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphistry\n",
    "graphistry.register(api=3, protocol = 'https', server='hub.graphistry.com', personal_key_id=os.getenv(\"GRAPHISTRY_KEY_ID\"), personal_key_secret=os.getenv(\"GRAPHISTRY_KEY_SECRET\"))\n",
    "g = graphistry.edges(graphDf, 'source','target')\n",
    "g.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms.connectivity import local_edge_connectivity\n",
    "\n",
    "\n",
    "# Create a sample graph\n",
    "G = nx.Graph()\n",
    "# G.add_edges_from([(1, 2), (1, 3), (2, 3), (2, 4), (3, 4), (4,5),(5,6),(8,10)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nxPairs = [(e['source'][0],e['target'][0], {'weight':e['target'][1]}) for e in allPairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edges_from(nxPairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.resistance_distance(G, 'spider','insect')\n",
    "\n",
    "# similarity1 = nx.simrank_similarity(G, 'haunting', 'haunting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordSimilarityAgainstList(targetWord: str, vectorizer, vSearchList, searchList):\n",
    "    vTargetWord = vectorizer.transform([targetWord]).toarray()\n",
    "    \n",
    "    maxSim = 0\n",
    "    maxWord = ''\n",
    "    for word, v_word in zip(searchList, vSearchList):\n",
    "        cosSim = cosine_similarity(vTargetWord, v_word)[0][0]\n",
    "        if cosSim > maxSim:\n",
    "            maxSim = cosSim\n",
    "            maxWord = word\n",
    "    return maxWord, maxSim\n",
    "\n",
    "\n",
    "def stringSimilarity(searchString, referenceString, vectorizer):\n",
    "    vector1 = vectorizer.fit_transform([searchString])\n",
    "    vector2 = vectorizer.transform([referenceString])\n",
    "\n",
    "    cosSim = cosine_similarity(vector1, vector2)[0][0]\n",
    "\n",
    "    return cosSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### These should be left in the other\n",
    "r9 = redis.Redis(\n",
    "    host='127.0.0.1',\n",
    "    port=6379,\n",
    "    encoding=\"utf-8\",\n",
    "    decode_responses=True,\n",
    "    db=9\n",
    ")\n",
    "\n",
    "keys = r9.keys('*')\n",
    "plots = r9.mget(keys)\n",
    "\n",
    "idPlotDict = dict(zip(keys,plots))\n",
    "ks = ['Story','Plot','Summary','Premise','Synopsis','Narratives']\n",
    "idPlotDictFiltered = {i:p for i,p in idPlotDict.items() if any(k in p for k in ks)}\n",
    "print(f\"WE FOUND: {len(idPlotDictFiltered)} PLOT SUMMARIES!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicDf = pd.read_csv('../data/tmdbDetails.csv')\n",
    "\n",
    "topicDf.dropna(subset='genres',inplace=True)\n",
    "topicDf = topicDf[topicDf['genres'].str.contains('Horror')]\n",
    "topicDf.dropna(subset='overview',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicDf['year'] = pd.to_datetime(topicDf['release_date']).dt.year\n",
    "titlesAndYears = topicDf[['title','year']]\n",
    "topicDf['plot'] = topicDf['imdb_id'].map(idPlotDictFiltered)\n",
    "topicDf['allText'] = topicDf['plot'] + ' ' + topicDf['overview']\n",
    "overviews = list(set(topicDf['plot'].values.tolist()))\n",
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUTOFF BETWEEN MODULE 1 and MODULE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicModels = pd.read_csv(\"../data/horrorTaxonomy.csv\")\n",
    "\n",
    "wordGroups = {}\n",
    "\n",
    "for e in topicModels.to_dict('records'):\n",
    "    valueLit = ast.literal_eval(e['value'])\n",
    "    if valueLit not in wordGroups.keys():\n",
    "        wordGroups[valueLit] = []\n",
    "    \n",
    "    wordGroups[valueLit].append(ast.literal_eval(e['target']))\n",
    "\n",
    "for k,v in wordGroups.items():\n",
    "    wordGroups[k] = list(set(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictCategory(plot, db: bool = False, title: str = ''):\n",
    "    plotClean = plot.translate(translator).lower()\n",
    "    plotSingular = makeSingular(plotClean)\n",
    "    plotWords = set(plotSingular.split(' '))\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range = (4,4))\n",
    "\n",
    "    sims = {}\n",
    "    for category, syns in wordGroups.items():\n",
    "\n",
    "        # for word in plotWords --> This gives us exact matches with proximity weightings, so it's much more reliable\n",
    "        catSum = sum([w[1] for w in set(syns) if w[0] in plotWords]) # change to character cosine sim\n",
    "        weightedInt = catSum / len(syns)\n",
    "\n",
    "        # This gives us imprecise matches, still useful e.g., vampiric vs. vampire, and it saves us having to lemmatize/stem\n",
    "        synWords = [w[0] for w in syns]\n",
    "        stringSim = stringSimilarity(searchString = plotSingular, referenceString = ' '.join(synWords), vectorizer = vectorizer)\n",
    "\n",
    "        totalCatSim = stringSim + (( 3 * weightedInt ) / len(plotWords) )\n",
    "        \n",
    "        sims[category[0]] = totalCatSim\n",
    "    \n",
    "    category = max(sims, key=sims.get)\n",
    "    \n",
    "    if db:\n",
    "        print('='*(len(title) + 12))\n",
    "        print(f\"SCORES FOR: {title}\")\n",
    "        print('-'*(len(title) + 12))\n",
    "        print(tabulate(sims.items(), headers=['Category','Score']))\n",
    "        print('='*(len(title) + 12))\n",
    "        print('')\n",
    "\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "\n",
    "for imdbId, plot in tqdm(idPlotDictFiltered.items()):\n",
    "    category = predictCategory(plot)\n",
    "\n",
    "    category['imdbId'] = imdbId\n",
    "\n",
    "    categories.append(category)\n",
    "\n",
    "catDf = pd.DataFrame.from_dict(categories)\n",
    "\n",
    "catDf.set_index('imdbId', inplace=True, drop=True)\n",
    "catDf.head()\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "catDfScaled = pd.DataFrame(scaler.fit_transform(catDf), columns = catDf.columns)\n",
    "catDfScaled.index = catDf.index\n",
    "catDfScaled['classification'] = catDfScaled.idxmax(axis=1)\n",
    "catDfScaled.sort_values(by='haunting', ascending=False).head(10)\n",
    "\n",
    "catDfScaled.groupby('classification').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdbDfRaw = pd.read_csv('../data/tmdbDetails.csv')\n",
    "idNameMap = tmdbDfRaw.set_index('imdb_id')['title'].to_dict()\n",
    "\n",
    "catDfScaled['title'] = catDfScaled.index.map(idNameMap)\n",
    "\n",
    "catDfScaled.dropna(subset='title', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catDfScaled.sort_index(inplace=True)\n",
    "len(catDfScaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
